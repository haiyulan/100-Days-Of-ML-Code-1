{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Day 1_Data_Preprocessing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haiyulan/100-Days-Of-ML-Code-1/blob/branch-of-100-Days-Of-ML-Code/Code/Day%201_Data_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjYLIgRhaY_K",
        "colab_type": "text"
      },
      "source": [
        "# 机器学习100天——第1天：数据预处理（Data Preprocessing）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5N3vJcgaY_M",
        "colab_type": "text"
      },
      "source": [
        "搭建anaconda环境，参考 https://zhuanlan.zhihu.com/p/33358809\n",
        "\n",
        "## 第一步：导入需要的库\n",
        "这两个是我们每次都需要导入的库。NumPy包含数学计算函数。Pandas用于导入和管理数据集。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94k2F9HdaY_N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bo0I4bgYaY_Q",
        "colab_type": "text"
      },
      "source": [
        "## 第二步：导入数据集\n",
        "数据集通常是.csv格式。CSV文件以文本形式保存表格数据。文件的每一行是一条数据记录。我们使用Pandas的read_csv方法读取本地csv文件为一个数据帧。然后，从数据帧中制作自变量和因变量的矩阵和向量。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNCoT4r0aY_R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "0e8378cc-b2a7-47e8-fa7b-86a8a912ea53"
      },
      "source": [
        "! git clone git@github.com:haiyulan/100-Days-Of-ML-Code-1.git\n",
        "\n",
        "! ls\n",
        "\n",
        "! ls 100-Days-Of-ML-Code-1/\n",
        "\n",
        "#base_dir = '100-Days-Of-ML-Code-1'\n",
        "dataset = pd.read_csv('100-Days-Of-ML-Code-1/datasets/Data.csv')\n",
        "# 不包括最后一列的所有列\n",
        "X = dataset.iloc[ : , :-1].values\n",
        "#取最后一列\n",
        "Y = dataset.iloc[ : , 3].values\n",
        "print(\"Step 2: Importing dataset\")\n",
        "print(\"X\")\n",
        "print(X)\n",
        "print(\"Y\")\n",
        "print(Y)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path '100-Days-Of-ML-Code-1' already exists and is not an empty directory.\n",
            "100-Days-Of-ML-Code-1  sample_data\n",
            " Code\t    FAQ.MD\t  LICENSE       README.md\n",
            " datasets   Info-graphs  'Other Docs'  'Translation specification.MD'\n",
            "Step 2: Importing dataset\n",
            "X\n",
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 nan]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' nan 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n",
            "Y\n",
            "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vf43C_SfaY_Y",
        "colab_type": "text"
      },
      "source": [
        "## 第三步：处理丢失数据\n",
        "我们得到的数据很少是完整的。数据可能因为各种原因丢失，为了不降低机器学习模型的性能，需要处理数据。我们可以用整列的平均值或中间值替换丢失的数据。我们用sklearn.preprocessing库中的Imputer类完成这项任务。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiJq-cz2aY_Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "f320bb82-a39b-4a65-8080-d2c173a0cb72"
      },
      "source": [
        "from sklearn.preprocessing import Imputer\n",
        "# axis=0表示按列进行\n",
        "imputer = Imputer(missing_values = \"NaN\", strategy = \"mean\", axis = 0)\n",
        "imputer = imputer.fit(X[ : , 1:3])\n",
        "X[ : , 1:3] = imputer.transform(X[ : , 1:3])\n",
        "print(\"---------------------\")\n",
        "print(\"Step 3: Handling the missing data\")\n",
        "print(\"step2\")\n",
        "print(\"X\")\n",
        "print(X)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------\n",
            "Step 3: Handling the missing data\n",
            "step2\n",
            "X\n",
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 63777.77777777778]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' 38.77777777777778 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
            "  warnings.warn(msg, category=DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXoSeK06aY_c",
        "colab_type": "text"
      },
      "source": [
        "## 第四步：解析分类数据\n",
        "分类数据指的是含有标签值而不是数字值的变量。取值范围通常是固定的。例如\"Yes\"和\"No\"不能用于模型的数学计算，所以需要解析成数字。为实现这一功能，我们从sklearn.preprocessing库导入LabelEncoder类。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0ZJMBEBaY_d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "61224303-27ac-4e34-c7e0-3470d76fcf72"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "labelencoder_X = LabelEncoder()\n",
        "X[ : , 0] = labelencoder_X.fit_transform(X[ : , 0])\n",
        "#Creating a dummy variable\n",
        "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
        "X = onehotencoder.fit_transform(X).toarray()\n",
        "labelencoder_Y = LabelEncoder()\n",
        "Y =  labelencoder_Y.fit_transform(Y)\n",
        "print(\"---------------------\")\n",
        "print(\"Step 4: Encoding categorical data\")\n",
        "print(\"X\")\n",
        "print(X)\n",
        "print(\"Y\")\n",
        "print(Y)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------\n",
            "Step 4: Encoding categorical data\n",
            "X\n",
            "[[1.00000000e+00 0.00000000e+00 0.00000000e+00 4.40000000e+01\n",
            "  7.20000000e+04]\n",
            " [0.00000000e+00 0.00000000e+00 1.00000000e+00 2.70000000e+01\n",
            "  4.80000000e+04]\n",
            " [0.00000000e+00 1.00000000e+00 0.00000000e+00 3.00000000e+01\n",
            "  5.40000000e+04]\n",
            " [0.00000000e+00 0.00000000e+00 1.00000000e+00 3.80000000e+01\n",
            "  6.10000000e+04]\n",
            " [0.00000000e+00 1.00000000e+00 0.00000000e+00 4.00000000e+01\n",
            "  6.37777778e+04]\n",
            " [1.00000000e+00 0.00000000e+00 0.00000000e+00 3.50000000e+01\n",
            "  5.80000000e+04]\n",
            " [0.00000000e+00 0.00000000e+00 1.00000000e+00 3.87777778e+01\n",
            "  5.20000000e+04]\n",
            " [1.00000000e+00 0.00000000e+00 0.00000000e+00 4.80000000e+01\n",
            "  7.90000000e+04]\n",
            " [0.00000000e+00 1.00000000e+00 0.00000000e+00 5.00000000e+01\n",
            "  8.30000000e+04]\n",
            " [1.00000000e+00 0.00000000e+00 0.00000000e+00 3.70000000e+01\n",
            "  6.70000000e+04]]\n",
            "Y\n",
            "[0 1 0 0 1 1 0 1 0 1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
            "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fICWqrhYaY_g",
        "colab_type": "text"
      },
      "source": [
        "## 第五步：拆分数据集为测试集合和训练集合\n",
        "把数据集拆分成两个：一个是用来训练模型的训练集合，另一个是用来验证模型的测试集合。两者比例一般是80:20。我们导入sklearn.model_selection库中的train_test_split()方法。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE54dxR-aY_g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "0311f3a4-1516-401f-8bae-ca752d7c420c"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split( X , Y , test_size = 0.2, random_state = 0)\n",
        "print(\"---------------------\")\n",
        "print(\"Step 5: Splitting the datasets into training sets and Test sets\")\n",
        "print(\"X_train\")\n",
        "print(X_train)\n",
        "print(\"X_test\")\n",
        "print(X_test)\n",
        "print(\"Y_train\")\n",
        "print(Y_train)\n",
        "print(\"Y_test\")\n",
        "print(Y_test)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------\n",
            "Step 5: Splitting the datasets into training sets and Test sets\n",
            "X_train\n",
            "[[0.00000000e+00 1.00000000e+00 0.00000000e+00 4.00000000e+01\n",
            "  6.37777778e+04]\n",
            " [1.00000000e+00 0.00000000e+00 0.00000000e+00 3.70000000e+01\n",
            "  6.70000000e+04]\n",
            " [0.00000000e+00 0.00000000e+00 1.00000000e+00 2.70000000e+01\n",
            "  4.80000000e+04]\n",
            " [0.00000000e+00 0.00000000e+00 1.00000000e+00 3.87777778e+01\n",
            "  5.20000000e+04]\n",
            " [1.00000000e+00 0.00000000e+00 0.00000000e+00 4.80000000e+01\n",
            "  7.90000000e+04]\n",
            " [0.00000000e+00 0.00000000e+00 1.00000000e+00 3.80000000e+01\n",
            "  6.10000000e+04]\n",
            " [1.00000000e+00 0.00000000e+00 0.00000000e+00 4.40000000e+01\n",
            "  7.20000000e+04]\n",
            " [1.00000000e+00 0.00000000e+00 0.00000000e+00 3.50000000e+01\n",
            "  5.80000000e+04]]\n",
            "X_test\n",
            "[[0.0e+00 1.0e+00 0.0e+00 3.0e+01 5.4e+04]\n",
            " [0.0e+00 1.0e+00 0.0e+00 5.0e+01 8.3e+04]]\n",
            "Y_train\n",
            "[1 1 1 0 1 0 0 1]\n",
            "Y_test\n",
            "[0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eOPMJyoaY_j",
        "colab_type": "text"
      },
      "source": [
        "## 第六步：特征量化\n",
        "大部分模型算法使用两点间的欧氏距离表示，但此特征在幅度、单位和范围姿态问题上变化很大。在距离计算中，高幅度的特征比低幅度特征权重更大。可用特征标准化或Z值归一化解决。导入sklearn.preprocessing库的StandardScalar类。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhFoDKrUaY_j",
        "colab_type": "code",
        "colab": {},
        "outputId": "eefe652b-cd53-4b7b-c530-c8dbdc4b6ed7"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc_X = StandardScaler()\n",
        "X_train = sc_X.fit_transform(X_train)\n",
        "X_test = sc_X.transform(X_test)\n",
        "print(\"---------------------\")\n",
        "print(\"Step 6: Feature Scaling\")\n",
        "print(\"X_train\")\n",
        "print(X_train)\n",
        "print(\"X_test\")\n",
        "print(X_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------\n",
            "Step 6: Feature Scaling\n",
            "X_train\n",
            "[[-1.          2.64575131 -0.77459667  0.26306757  0.12381479]\n",
            " [ 1.         -0.37796447 -0.77459667 -0.25350148  0.46175632]\n",
            " [-1.         -0.37796447  1.29099445 -1.97539832 -1.53093341]\n",
            " [-1.         -0.37796447  1.29099445  0.05261351 -1.11141978]\n",
            " [ 1.         -0.37796447 -0.77459667  1.64058505  1.7202972 ]\n",
            " [-1.         -0.37796447  1.29099445 -0.0813118  -0.16751412]\n",
            " [ 1.         -0.37796447 -0.77459667  0.95182631  0.98614835]\n",
            " [ 1.         -0.37796447 -0.77459667 -0.59788085 -0.48214934]]\n",
            "X_test\n",
            "[[ 0.  0.  0. -1. -1.]\n",
            " [ 0.  0.  0.  1.  1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hr374gGKaY_n",
        "colab_type": "text"
      },
      "source": [
        "<b>完整的项目请前往Github项目<a href=\"https://github.com/MachineLearning100/100-Days-Of-ML-Code\">100-Days-Of-ML-Code</a>查看。有任何的建议或者意见欢迎在issue中提出~</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQp6pVGJaY_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}